[
    {
        "paper_title": "Synthesizer: Rethinking Self-Attention in Transformer Models",
        "arxiv_id": "2005.00743"
    },
    {
        "paper_title": "Hand-crafted Attention is All You Need? A Study of Attention on Self-supervised Audio Transformer",
        "arxiv_id": "2006.05174"
    },
    {
        "paper_title": "Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel",
        "arxiv_id": "1908.11775",
        "link": "https://github.com/yaohungt/TransformerDissection"
    },
    {
        "paper_title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
        "arxiv_id": "2006.16236",
        "link": "https://github.com/idiap/fast-transformers"
    }
]
