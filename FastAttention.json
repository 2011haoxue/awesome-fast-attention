[
    {
        "paper_title": "Large Memory Layers with Product Keys",
        "arxiv_id": "1907.05242",
        "link": "https://github.com/facebookresearch/XLM"
    },
    {
        "paper_title": "Linear Attention Mechanism: An Efficient Attention for Semantic Segmentation",
        "arxiv_id": "2007.14902",
        "link": "https://github.com/lironui/Linear-Attention-Mechanism"
    },
    {
        "paper_title": "Linformer: Self-Attention with Linear Complexity",
        "arxiv_id": "2006.04768",
        "link": "https://github.com/lucidrains/linformer"
    },
    {
        "paper_title": "Longformer: The Long-Document Transformer",
        "arxiv_id": "2004.05150",
        "link": "https://github.com/allenai/longformer"
    },
    {
        "paper_title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers",
        "arxiv_id": "2006.03555"
    },
    {
        "paper_title": "Memory Transformer",
        "arxiv_id": "2006.11527"
    },
    {
        "paper_title": "Multi-scale Transformer Language Models",
        "arxiv_id": "2005.00581"
    },
    {
        "paper_title": "Permutohedral Attention Module for Efficient Non-Local Neural Networks",
        "arxiv_id": "1907.00641",
        "link": "https://github.com/SamuelJoutard/Permutohedral_attention_module"
    },
    {
        "paper_title": "Processing Megapixel Images with Deep Attention-Sampling Models",
        "arxiv_id": "1905.03711",
        "link": "https://github.com/idiap/attention-sampling"
    },
    {
        "paper_title": "Reformer: The Efficient Transformer",
        "arxiv_id": "2001.04451",
        "link": "https://github.com/google/trax/tree/master/trax/models/reformer"
    },
    {
        "paper_title": "SANet:Superpixel Attention Network for Skin Lesion Attributes Detection",
        "arxiv_id": "1910.08995",
        "link": "https://github.com/bermanmaxim/superpixPool"
    },
    {
        "paper_title": "SCRAM: Spatially Coherent Randomized Attention Maps",
        "arxiv_id": "1905.10308"
    },
    {
        "paper_title": "Sparse Sinkhorn Attention",
        "arxiv_id": "2002.11296",
        "link": "https://github.com/lucidrains/sinkhorn-transformer"
    },
    {
        "paper_title": "Star-Transformer",
        "arxiv_id": "1902.09113",
        "link": "https://github.com/fastnlp/fastNLP/blob/master/fastNLP/modules/encoder/star_transformer.py"
    },
    {
        "paper_title": "Synthesizer: Rethinking Self-Attention in Transformer Models",
        "arxiv_id": "2005.00743"
    },
    {
        "paper_title": "Hand-crafted Attention is All You Need? A Study of Attention on Self-supervised Audio Transformer",
        "arxiv_id": "2006.05174"
    },
    {
        "paper_title": "Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel",
        "arxiv_id": "1908.11775",
        "link": "https://github.com/yaohungt/TransformerDissection"
    },
    {
        "paper_title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
        "arxiv_id": "1901.02860",
        "link": "https://github.com/kimiyoung/transformer-xl"
    },
    {
        "paper_title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
        "arxiv_id": "2006.16236",
        "link": "https://github.com/idiap/fast-transformers"
    }
]
